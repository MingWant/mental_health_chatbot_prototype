"""
RAGç³»çµ±åˆ†å¡Šç­–ç•¥é›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•å°‡æ–°çš„åˆ†å¡Šç­–ç•¥æ•´åˆåˆ°ç¾æœ‰çš„å¿ƒç†å¥åº·RAGç³»çµ±ä¸­
"""

import asyncio
import os
import uuid
import aiofiles
from datetime import datetime
from typing import List, Dict, Any, Optional
from enhanced_chunking_strategies import (
    EnhancedChunkingStrategies, 
    ChunkingStrategy, 
    ChunkConfig
)
from mental_health_rag_service import MentalHealthRAGService

class EnhancedMentalHealthRAGService(MentalHealthRAGService):
    """å¢žå¼·ç‰ˆå¿ƒç†å¥åº·RAGæœå‹™ï¼Œæ”¯æŒå¤šç¨®åˆ†å¡Šç­–ç•¥"""
    
    def __init__(self):
        super().__init__()
        self.chunking_strategies = EnhancedChunkingStrategies()
    
    async def upload_and_process_document_enhanced(
        self, 
        file_content: bytes, 
        filename: str, 
        *,
        chunking_strategy: ChunkingStrategy = ChunkingStrategy.SEMANTIC,
        chunk_size: int = 200,
        overlap: int = 30,
        mode: str = "sentences",
        custom_keywords: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨å¢žå¼·åˆ†å¡Šç­–ç•¥ä¸Šå‚³å’Œè™•ç†æ–‡æª”"""
        
        # ç”Ÿæˆå”¯ä¸€æ–‡æª”ID
        doc_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(self.upload_dir, f"{doc_id}_{filename}")
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(file_content)
        
        try:
            # æå–æ–‡æœ¬å…§å®¹
            text_content = await self.doc_processor._extract_text(file_path, os.path.splitext(filename)[1].lower())

            # çµæ§‹å‹å¥½çš„æ¸…ç†ï¼šä¿ç•™æ›è¡Œèˆ‡æ®µè½ï¼Œä»¥æ”¯æ´ hierarchical/session åˆ†å¡Š
            def _clean_text_preserve_structure(raw: str) -> str:
                # Normalize newlines
                raw = raw.replace('\r\n', '\n').replace('\r', '\n')
                # Collapse excessive spaces within lines but keep line breaks
                lines = [
                    ' '.join(line.strip().split()) for line in raw.split('\n')
                ]
                # Remove consecutive more than 2 empty lines
                normalized = []
                empty_run = 0
                for line in lines:
                    if line == '':
                        empty_run += 1
                        if empty_run <= 2:
                            normalized.append('')
                    else:
                        empty_run = 0
                        normalized.append(line)
                return '\n'.join(normalized).strip()

            cleaned_text = _clean_text_preserve_structure(text_content)
            
            # åˆ†é¡žå…§å®¹
            categories = self.doc_processor._classify_content(cleaned_text, custom_keywords=custom_keywords)
            
            # ä½¿ç”¨å¢žå¼·åˆ†å¡Šç­–ç•¥
            config = ChunkConfig(
                strategy=chunking_strategy,
                chunk_size=chunk_size,
                overlap=overlap,
                mode=mode
            )
            
            chunks = self.chunking_strategies.chunk_text(cleaned_text, config)
            
            # æº–å‚™å…ƒæ•¸æ“š
            metadata = {
                "filename": filename,
                "extension": os.path.splitext(filename)[1].lower(),
                "categories": categories,
                "doc_id": doc_id,
                "chunking_strategy": chunking_strategy.value,
                "chunking_strategy": chunking_strategy.value,
                "chunk_size": chunk_size,
                "overlap": overlap,
                "mode": mode
            }
            
            # æ·»åŠ åˆ°å‘é‡æ•¸æ“šåº«
            success = await self.vector_db.add_document(
                doc_id=doc_id,
                chunks=chunks,
                metadata=metadata
            )
            
            if success:
                # åˆªé™¤è‡¨æ™‚æ–‡ä»¶
                os.remove(file_path)
                
                return {
                    "success": True,
                    "doc_id": doc_id,
                    "filename": filename,
                    "categories": categories,
                    "chunk_count": len(chunks),
                    "chunking_strategy": chunking_strategy.value,
                    "processed_at": datetime.now().isoformat(),
                    "message": f"æ–‡æª”ä½¿ç”¨ {chunking_strategy.value} ç­–ç•¥æˆåŠŸè™•ç†",
                    "chunk_details": {
                        "avg_length": sum(chunk['length'] for chunk in chunks) / len(chunks) if chunks else 0,
                        "chunk_types": list(set(chunk.get('chunk_type', 'default') for chunk in chunks)),
                        "strategy_used": chunking_strategy.value
                    }
                }
            else:
                os.remove(file_path)
                return {
                    "success": False,
                    "message": "å‘é‡åŒ–è™•ç†å¤±æ•—"
                }
                
        except Exception as e:
            if os.path.exists(file_path):
                os.remove(file_path)
            return {
                "success": False,
                "message": f"æ–‡æª”è™•ç†å¤±æ•—: {str(e)}"
            }

async def demonstrate_chunking_comparison():
    """æ¼”ç¤ºä¸åŒåˆ†å¡Šç­–ç•¥çš„æ¯”è¼ƒ"""
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = """
# å­¸ç”Ÿå¿ƒç†å¥åº·æŒ‡å—

## ç¬¬ä¸€ç« ï¼šèªè­˜æƒ…ç·’

æƒ…ç·’æ˜¯æˆ‘å€‘å°å¤–ç•Œåˆºæ¿€çš„è‡ªç„¶åæ‡‰ã€‚å­¸æœƒè­˜åˆ¥å’Œç®¡ç†æƒ…ç·’æ˜¯å¿ƒç†å¥åº·çš„é‡è¦æŠ€èƒ½ã€‚

### 1.1 å¸¸è¦‹æƒ…ç·’é¡žåž‹

**ç„¦æ…®æƒ…ç·’**ï¼šç•¶é¢è‡¨è€ƒè©¦ã€æ¼”è¬›æˆ–é‡è¦æ±ºç­–æ™‚ï¼Œæˆ‘å€‘å¯èƒ½æœƒæ„Ÿåˆ°ç„¦æ…®ã€‚é€™æ˜¯æ­£å¸¸çš„åæ‡‰ã€‚

**æŠ‘é¬±æƒ…ç·’**ï¼šé•·æ™‚é–“æ„Ÿåˆ°æ‚²å‚·ã€å¤±åŽ»èˆˆè¶£æˆ–å¸Œæœ›ï¼Œå¯èƒ½æ˜¯æŠ‘é¬±çš„å¾µå…†ã€‚

**æ†¤æ€’æƒ…ç·’**ï¼šç•¶äº‹æƒ…ä¸å¦‚é æœŸæ™‚ï¼Œæˆ‘å€‘å¯èƒ½æœƒæ„Ÿåˆ°æ†¤æ€’ã€‚å­¸æœƒè¡¨é”æ†¤æ€’è€Œä¸å‚·å®³ä»–äººå¾ˆé‡è¦ã€‚

## ç¬¬äºŒç« ï¼šå£“åŠ›ç®¡ç†æŠ€å·§

### 2.1 æ·±å‘¼å¸ç·´ç¿’

æ·±å‘¼å¸æ˜¯ç°¡å–®æœ‰æ•ˆçš„å£“åŠ›ç®¡ç†æŠ€å·§ï¼š
1. æ‰¾ä¸€å€‹å®‰éœçš„åœ°æ–¹åä¸‹
2. é–‰ä¸Šçœ¼ç›ï¼Œå°ˆæ³¨æ–¼å‘¼å¸
3. å¸æ°£4ç§’ï¼Œå±æ°£4ç§’ï¼Œå‘¼æ°£6ç§’
4. é‡è¤‡5-10æ¬¡

### 2.2 æ™‚é–“ç®¡ç†

è‰¯å¥½çš„æ™‚é–“ç®¡ç†å¯ä»¥æ¸›å°‘å£“åŠ›ï¼š
- åˆ¶å®šæ¯æ—¥è¨ˆåŠƒ
- å„ªå…ˆè™•ç†é‡è¦ä»»å‹™
- å­¸æœƒèªª"ä¸"
- çµ¦è‡ªå·±ç•™å‡ºä¼‘æ¯æ™‚é–“

## ç¬¬ä¸‰ç« ï¼šå°‹æ±‚å¹«åŠ©

ç•¶è‡ªæˆ‘èª¿ç¯€ç„¡æ³•è§£æ±ºå•é¡Œæ™‚ï¼Œå°‹æ±‚å°ˆæ¥­å¹«åŠ©æ˜¯æ˜Žæ™ºçš„é¸æ“‡ã€‚

### 3.1 æ ¡åœ’è³‡æº

å¤§å¤šæ•¸å­¸æ ¡éƒ½æä¾›å¿ƒç†å¥åº·æœå‹™ï¼š
- å¿ƒç†è«®è©¢ä¸­å¿ƒ
- å­¸ç”Ÿå¥åº·æœå‹™
- åŒè¼©æ”¯æŒå°çµ„

### 3.2 ä½•æ™‚å°‹æ±‚å¹«åŠ©

å¦‚æžœå‡ºç¾ä»¥ä¸‹æƒ…æ³ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­å¹«åŠ©ï¼š
- æƒ…ç·’æŒçºŒä½Žè½è¶…éŽå…©é€±
- å½±éŸ¿æ—¥å¸¸å­¸ç¿’å’Œç”Ÿæ´»
- æœ‰è‡ªå‚·æˆ–å‚·å®³ä»–äººçš„æƒ³æ³•
- ç¡çœ æˆ–é£²é£Ÿç¿’æ…£ç™¼ç”Ÿé‡å¤§è®ŠåŒ–
"""
    
    # å‰µå»ºå¢žå¼·RAGæœå‹™
    enhanced_rag = EnhancedMentalHealthRAGService()
    
    # æ¸¬è©¦ä¸åŒåˆ†å¡Šç­–ç•¥
    strategies_to_test = [
        (ChunkingStrategy.FIXED_LENGTH, "å›ºå®šé•·åº¦åˆ†å¡Š"),
        (ChunkingStrategy.SEMANTIC, "èªžç¾©åˆ†å¡Šï¼ˆå¥å­ï¼‰"),
        (ChunkingStrategy.HIERARCHICAL, "å±¤æ¬¡åˆ†å¡Š"),
        (ChunkingStrategy.ADAPTIVE, "è‡ªé©æ‡‰åˆ†å¡Š")
    ]
    
    print("=== RAGç³»çµ±åˆ†å¡Šç­–ç•¥æ¯”è¼ƒæ¼”ç¤º ===\n")
    
    for strategy, description in strategies_to_test:
        print(f"ðŸ“‹ {description}")
        print("-" * 50)
        
        # å‰µå»ºåˆ†å¡Šé…ç½®
        config = ChunkConfig(
            strategy=strategy,
            chunk_size=200,
            overlap=30,
            mode="sentences" if strategy == ChunkingStrategy.SEMANTIC else "chars"
        )
        
        # åŸ·è¡Œåˆ†å¡Š
        chunks = enhanced_rag.chunking_strategies.chunk_text(sample_text, config)
        
        # é¡¯ç¤ºçµæžœ
        print(f"ç¸½åˆ†å¡Šæ•¸: {len(chunks)}")
        print(f"å¹³å‡åˆ†å¡Šé•·åº¦: {sum(chunk['length'] for chunk in chunks) / len(chunks):.1f} å­—ç¬¦")
        print(f"åˆ†å¡Šé¡žåž‹: {list(set(chunk.get('chunk_type', 'default') for chunk in chunks))}")
        
        # é¡¯ç¤ºå‰3å€‹åˆ†å¡Šçš„å…§å®¹
        print("\nå‰3å€‹åˆ†å¡Šå…§å®¹:")
        for i, chunk in enumerate(chunks[:3]):
            print(f"\nåˆ†å¡Š {i+1} ({chunk.get('chunk_type', 'default')}):")
            print(f"é•·åº¦: {chunk['length']} å­—ç¬¦")
            print(f"å…§å®¹: {chunk['text'][:100]}...")
        
        print("\n" + "="*60 + "\n")

async def demonstrate_session_chunking():
    """æ¼”ç¤ºæœƒè©±åˆ†å¡Šçš„ç‰¹æ®Šç”¨æ³•"""
    
    # æ¨¡æ“¬æœƒè©±è¨˜éŒ„
    session_text = """
æœƒè­°è¨˜éŒ„ - å¿ƒç†å¥åº·ç ”è¨Žæœƒ
æ™‚é–“: 2024-01-15 14:00-16:00

ä¸»æŒäºº: å¤§å®¶å¥½ï¼Œæ­¡è¿ŽåƒåŠ ä»Šå¤©çš„å¿ƒç†å¥åº·ç ”è¨Žæœƒã€‚æˆ‘å€‘ä»Šå¤©çš„ä¸»é¡Œæ˜¯"å­¸ç”Ÿå£“åŠ›ç®¡ç†"ã€‚

Speaker 1: æˆ‘æƒ³åˆ†äº«ä¸€äº›é—œæ–¼è€ƒè©¦ç„¦æ…®çš„ç¶“é©—ã€‚ç•¶æˆ‘é¢è‡¨é‡è¦è€ƒè©¦æ™‚ï¼Œæˆ‘æœƒæ„Ÿåˆ°éžå¸¸ç·Šå¼µã€‚

Speaker 2: æˆ‘ä¹Ÿæœ‰é¡žä¼¼çš„ç¶“æ­·ã€‚æˆ‘ç™¼ç¾æ·±å‘¼å¸ç·´ç¿’å°æˆ‘å¾ˆæœ‰å¹«åŠ©ã€‚

ä¸»æŒäºº: å¾ˆå¥½çš„åˆ†äº«ã€‚è®“æˆ‘å€‘è¨Žè«–ä¸€äº›å…·é«”çš„æ‡‰å°ç­–ç•¥ã€‚

Q1: å¦‚ä½•å€åˆ†æ­£å¸¸çš„è€ƒè©¦ç·Šå¼µå’Œéœ€è¦å°ˆæ¥­å¹«åŠ©çš„ç„¦æ…®ï¼Ÿ

Speaker 1: æˆ‘èªç‚ºé—œéµæ˜¯çœ‹é€™ç¨®æƒ…ç·’æ˜¯å¦å½±éŸ¿äº†æ—¥å¸¸åŠŸèƒ½ã€‚å¦‚æžœç„¡æ³•æ­£å¸¸å­¸ç¿’æˆ–ç”Ÿæ´»ï¼Œå°±æ‡‰è©²å°‹æ±‚å¹«åŠ©ã€‚

Speaker 2: æˆ‘åŒæ„ã€‚æŒçºŒæ™‚é–“ä¹Ÿå¾ˆé‡è¦ã€‚å¦‚æžœç„¦æ…®æŒçºŒè¶…éŽå…©é€±ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­å¹«åŠ©ã€‚

ä¸»æŒäºº: è¬è¬å¤§å®¶çš„åˆ†äº«ã€‚è®“æˆ‘å€‘ç¹¼çºŒè¨Žè«–å…¶ä»–è©±é¡Œã€‚
"""
    
    print("=== æœƒè©±åˆ†å¡Šæ¼”ç¤º ===\n")
    
    enhanced_rag = EnhancedMentalHealthRAGService()
    
    config = ChunkConfig(
        strategy=ChunkingStrategy.SESSION,
        chunk_size=300,
        overlap=50,
        mode="session"
    )
    
    chunks = enhanced_rag.chunking_strategies.chunk_text(session_text, config)
    
    print(f"æœƒè©±åˆ†å¡Šçµæžœ: {len(chunks)} å€‹åˆ†å¡Š")
    
    for i, chunk in enumerate(chunks):
        print(f"\næœƒè©±åˆ†å¡Š {i+1}:")
        print(f"é¡žåž‹: {chunk.get('chunk_type', 'unknown')}")
        print(f"é•·åº¦: {chunk['length']} å­—ç¬¦")
        print(f"å…§å®¹: {chunk['text'][:150]}...")

async def main():
    """ä¸»å‡½æ•¸ï¼šé‹è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ðŸš€ é–‹å§‹RAGç³»çµ±åˆ†å¡Šç­–ç•¥æ¼”ç¤º...\n")
    
    # æ¯”è¼ƒä¸åŒåˆ†å¡Šç­–ç•¥
    await demonstrate_chunking_comparison()
    
    # æ¼”ç¤ºæœƒè©±åˆ†å¡Š
    await demonstrate_session_chunking()
    
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
