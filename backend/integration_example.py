"""
å¯¦éš›é›†æˆç¤ºä¾‹ï¼šå°‡å¢å¼·åˆ†å¡Šç­–ç•¥æ•´åˆåˆ°ç¾æœ‰RAGç³»çµ±
"""

import asyncio
import os
from typing import Dict, Any
from enhanced_chunking_strategies import ChunkingStrategy, ChunkConfig
from mental_health_rag_service import MentalHealthRAGService

class IntegratedMentalHealthRAG:
    """æ•´åˆäº†å¢å¼·åˆ†å¡Šç­–ç•¥çš„å¿ƒç†å¥åº·RAGç³»çµ±"""
    
    def __init__(self):
        self.rag_service = MentalHealthRAGService()
        self.chunking_strategies = None
        
        # å˜—è©¦å°å…¥å¢å¼·åˆ†å¡Šç­–ç•¥
        try:
            from enhanced_chunking_strategies import EnhancedChunkingStrategies
            self.chunking_strategies = EnhancedChunkingStrategies()
            print("âœ… å¢å¼·åˆ†å¡Šç­–ç•¥å·²åŠ è¼‰")
        except ImportError as e:
            print(f"âš ï¸ å¢å¼·åˆ†å¡Šç­–ç•¥åŠ è¼‰å¤±æ•—: {e}")
            print("å°‡ä½¿ç”¨åŸæœ‰çš„å›ºå®šé•·åº¦åˆ†å¡Š")
    
    async def process_document_with_strategy(
        self, 
        file_path: str, 
        filename: str,
        strategy: ChunkingStrategy = ChunkingStrategy.SEMANTIC,
        **kwargs
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šç­–ç•¥è™•ç†æ–‡æª”"""
        
        if not self.chunking_strategies:
            # å›é€€åˆ°åŸæœ‰æ–¹æ³•
            return await self._fallback_processing(file_path, filename, **kwargs)
        
        try:
            # è®€å–æ–‡ä»¶å…§å®¹
            with open(file_path, 'rb') as f:
                file_content = f.read()
            
            # æå–æ–‡æœ¬
            text_content = await self.rag_service.doc_processor._extract_text(
                file_path, os.path.splitext(filename)[1].lower()
            )
            cleaned_text = self.rag_service.doc_processor._clean_text(text_content)
            
            # åˆ†é¡å…§å®¹
            categories = self.rag_service.doc_processor._classify_content(
                cleaned_text, kwargs.get('custom_keywords')
            )
            
            # ä½¿ç”¨å¢å¼·åˆ†å¡Šç­–ç•¥
            config = ChunkConfig(
                strategy=strategy,
                chunk_size=kwargs.get('chunk_size', 200),
                overlap=kwargs.get('overlap', 30),
                mode=kwargs.get('mode', 'sentences')
            )
            
            chunks = self.chunking_strategies.chunk_text(cleaned_text, config)
            
            return {
                "success": True,
                "filename": filename,
                "strategy_used": strategy.value,
                "categories": categories,
                "chunk_count": len(chunks),
                "chunks": chunks,
                "statistics": {
                    "avg_length": sum(chunk['length'] for chunk in chunks) / len(chunks) if chunks else 0,
                    "chunk_types": list(set(chunk.get('chunk_type', 'default') for chunk in chunks)),
                    "total_text_length": len(cleaned_text)
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "strategy_attempted": strategy.value
            }
    
    async def _fallback_processing(self, file_path: str, filename: str, **kwargs) -> Dict[str, Any]:
        """å›é€€åˆ°åŸæœ‰è™•ç†æ–¹æ³•"""
        try:
            result = await self.rag_service.doc_processor.process_file(
                file_path, filename,
                chunk_size=kwargs.get('chunk_size', 200),
                overlap=kwargs.get('overlap', 30),
                mode=kwargs.get('mode', 'chars'),
                custom_keywords=kwargs.get('custom_keywords')
            )
            result["strategy_used"] = "fixed_length_fallback"
            return result
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "strategy_attempted": "fixed_length_fallback"
            }

async def demonstrate_integration():
    """æ¼”ç¤ºæ•´åˆæ•ˆæœ"""
    
    print("ğŸš€ é–‹å§‹RAGç³»çµ±åˆ†å¡Šç­–ç•¥æ•´åˆæ¼”ç¤º...\n")
    
    # å‰µå»ºæ•´åˆRAGç³»çµ±
    integrated_rag = IntegratedMentalHealthRAG()
    
    # å‰µå»ºæ¸¬è©¦æ–‡æª”
    test_content = """
# å­¸ç”Ÿå¿ƒç†å¥åº·æŒ‡å—

## ç¬¬ä¸€ç« ï¼šèªè­˜æƒ…ç·’

æƒ…ç·’æ˜¯æˆ‘å€‘å°å¤–ç•Œåˆºæ¿€çš„è‡ªç„¶åæ‡‰ã€‚å­¸æœƒè­˜åˆ¥å’Œç®¡ç†æƒ…ç·’æ˜¯å¿ƒç†å¥åº·çš„é‡è¦æŠ€èƒ½ã€‚

### 1.1 å¸¸è¦‹æƒ…ç·’é¡å‹

**ç„¦æ…®æƒ…ç·’**ï¼šç•¶é¢è‡¨è€ƒè©¦ã€æ¼”è¬›æˆ–é‡è¦æ±ºç­–æ™‚ï¼Œæˆ‘å€‘å¯èƒ½æœƒæ„Ÿåˆ°ç„¦æ…®ã€‚é€™æ˜¯æ­£å¸¸çš„åæ‡‰ï¼Œä½†å¦‚æœæŒçºŒæ™‚é–“éé•·æˆ–å½±éŸ¿æ—¥å¸¸ç”Ÿæ´»ï¼Œå°±éœ€è¦é—œæ³¨ã€‚

**æŠ‘é¬±æƒ…ç·’**ï¼šé•·æ™‚é–“æ„Ÿåˆ°æ‚²å‚·ã€å¤±å»èˆˆè¶£æˆ–å¸Œæœ›ï¼Œå¯èƒ½æ˜¯æŠ‘é¬±çš„å¾µå…†ã€‚éœ€è¦åŠæ™‚å°‹æ±‚å°ˆæ¥­å¹«åŠ©ã€‚

**æ†¤æ€’æƒ…ç·’**ï¼šç•¶äº‹æƒ…ä¸å¦‚é æœŸæ™‚ï¼Œæˆ‘å€‘å¯èƒ½æœƒæ„Ÿåˆ°æ†¤æ€’ã€‚å­¸æœƒè¡¨é”æ†¤æ€’è€Œä¸å‚·å®³ä»–äººå¾ˆé‡è¦ã€‚

## ç¬¬äºŒç« ï¼šå£“åŠ›ç®¡ç†æŠ€å·§

### 2.1 æ·±å‘¼å¸ç·´ç¿’

æ·±å‘¼å¸æ˜¯ç°¡å–®æœ‰æ•ˆçš„å£“åŠ›ç®¡ç†æŠ€å·§ï¼š
1. æ‰¾ä¸€å€‹å®‰éœçš„åœ°æ–¹åä¸‹
2. é–‰ä¸Šçœ¼ç›ï¼Œå°ˆæ³¨æ–¼å‘¼å¸
3. å¸æ°£4ç§’ï¼Œå±æ°£4ç§’ï¼Œå‘¼æ°£6ç§’
4. é‡è¤‡5-10æ¬¡

### 2.2 æ™‚é–“ç®¡ç†

è‰¯å¥½çš„æ™‚é–“ç®¡ç†å¯ä»¥æ¸›å°‘å£“åŠ›ï¼š
- åˆ¶å®šæ¯æ—¥è¨ˆåŠƒ
- å„ªå…ˆè™•ç†é‡è¦ä»»å‹™
- å­¸æœƒèªª"ä¸"
- çµ¦è‡ªå·±ç•™å‡ºä¼‘æ¯æ™‚é–“

## ç¬¬ä¸‰ç« ï¼šå°‹æ±‚å¹«åŠ©

ç•¶è‡ªæˆ‘èª¿ç¯€ç„¡æ³•è§£æ±ºå•é¡Œæ™‚ï¼Œå°‹æ±‚å°ˆæ¥­å¹«åŠ©æ˜¯æ˜æ™ºçš„é¸æ“‡ã€‚

### 3.1 æ ¡åœ’è³‡æº

å¤§å¤šæ•¸å­¸æ ¡éƒ½æä¾›å¿ƒç†å¥åº·æœå‹™ï¼š
- å¿ƒç†è«®è©¢ä¸­å¿ƒ
- å­¸ç”Ÿå¥åº·æœå‹™
- åŒè¼©æ”¯æŒå°çµ„

### 3.2 ä½•æ™‚å°‹æ±‚å¹«åŠ©

å¦‚æœå‡ºç¾ä»¥ä¸‹æƒ…æ³ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­å¹«åŠ©ï¼š
- æƒ…ç·’æŒçºŒä½è½è¶…éå…©é€±
- å½±éŸ¿æ—¥å¸¸å­¸ç¿’å’Œç”Ÿæ´»
- æœ‰è‡ªå‚·æˆ–å‚·å®³ä»–äººçš„æƒ³æ³•
- ç¡çœ æˆ–é£²é£Ÿç¿’æ…£ç™¼ç”Ÿé‡å¤§è®ŠåŒ–
"""
    
    # ä¿å­˜æ¸¬è©¦æ–‡æª”
    test_file_path = "backend/test_mental_health_guide.txt"
    with open(test_file_path, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    print("ğŸ“„ æ¸¬è©¦æ–‡æª”å·²å‰µå»º")
    
    # æ¸¬è©¦ä¸åŒåˆ†å¡Šç­–ç•¥
    strategies_to_test = [
        (ChunkingStrategy.FIXED_LENGTH, "å›ºå®šé•·åº¦åˆ†å¡Š"),
        (ChunkingStrategy.SEMANTIC, "èªç¾©åˆ†å¡Š"),
        (ChunkingStrategy.HIERARCHICAL, "å±¤æ¬¡åˆ†å¡Š"),
        (ChunkingStrategy.ADAPTIVE, "è‡ªé©æ‡‰åˆ†å¡Š")
    ]
    
    results = {}
    
    for strategy, description in strategies_to_test:
        print(f"\nğŸ” æ¸¬è©¦ {description}...")
        
        result = await integrated_rag.process_document_with_strategy(
            test_file_path,
            "test_mental_health_guide.txt",
            strategy=strategy,
            chunk_size=200,
            overlap=30,
            mode="sentences" if strategy == ChunkingStrategy.SEMANTIC else "chars"
        )
        
        results[strategy.value] = result
        
        if result["success"]:
            print(f"âœ… {description} æˆåŠŸ")
            print(f"   - åˆ†å¡Šæ•¸é‡: {result['chunk_count']}")
            print(f"   - å¹³å‡é•·åº¦: {result['statistics']['avg_length']:.1f} å­—ç¬¦")
            print(f"   - åˆ†å¡Šé¡å‹: {result['statistics']['chunk_types']}")
            print(f"   - è­˜åˆ¥é¡åˆ¥: {', '.join(result['categories'][:3])}...")
        else:
            print(f"âŒ {description} å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    # æ¯”è¼ƒçµæœ
    print("\nğŸ“Š åˆ†å¡Šç­–ç•¥æ¯”è¼ƒçµæœ:")
    print("=" * 80)
    print(f"{'ç­–ç•¥':<15} {'åˆ†å¡Šæ•¸':<8} {'å¹³å‡é•·åº¦':<10} {'åˆ†å¡Šé¡å‹':<20} {'ç‹€æ…‹':<8}")
    print("-" * 80)
    
    for strategy_name, result in results.items():
        if result["success"]:
            chunk_types = ', '.join(result['statistics']['chunk_types'][:2])
            if len(result['statistics']['chunk_types']) > 2:
                chunk_types += "..."
            print(f"{strategy_name:<15} {result['chunk_count']:<8} "
                  f"{result['statistics']['avg_length']:<10.1f} "
                  f"{chunk_types:<20} {'âœ…':<8}")
        else:
            print(f"{strategy_name:<15} {'N/A':<8} {'N/A':<10} {'N/A':<20} {'âŒ':<8}")
    
    # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
    if os.path.exists(test_file_path):
        os.remove(test_file_path)
        print(f"\nğŸ§¹ æ¸¬è©¦æ–‡ä»¶å·²æ¸…ç†")
    
    print("\nğŸ‰ æ•´åˆæ¼”ç¤ºå®Œæˆï¼")
    
    return results

async def demonstrate_session_chunking():
    """æ¼”ç¤ºæœƒè©±åˆ†å¡Šçš„ç‰¹æ®Šæ‡‰ç”¨"""
    
    print("\nğŸ¤ æœƒè©±åˆ†å¡Šæ¼”ç¤º:")
    print("=" * 50)
    
    # æ¨¡æ“¬å¿ƒç†è«®è©¢è¨˜éŒ„
    session_text = """
å¿ƒç†è«®è©¢è¨˜éŒ„
æ—¥æœŸ: 2024-01-15
è«®è©¢å¸«: å¼µå¿ƒç†å¸«
ä¾†è¨ªè€…: å­¸ç”ŸA

14:00 - å¼µå¿ƒç†å¸«: ä½ å¥½ï¼Œä»Šå¤©æ„Ÿè¦ºæ€éº¼æ¨£ï¼Ÿ

14:01 - å­¸ç”ŸA: æˆ‘æœ€è¿‘å£“åŠ›å¾ˆå¤§ï¼Œè€ƒè©¦å¿«åˆ°äº†ï¼Œæ„Ÿè¦ºå¾ˆç„¦æ…®ã€‚

14:02 - å¼µå¿ƒç†å¸«: æˆ‘ç†è§£ä½ çš„æ„Ÿå—ã€‚è€ƒè©¦ç„¦æ…®æ˜¯å¾ˆå¸¸è¦‹çš„ã€‚èƒ½å…·é«”èªªèªªä½ çš„ç„¦æ…®è¡¨ç¾å—ï¼Ÿ

14:03 - å­¸ç”ŸA: ä¸»è¦æ˜¯ç¡ä¸å¥½ï¼Œæ³¨æ„åŠ›ä¸é›†ä¸­ï¼Œæœ‰æ™‚å€™æœƒå¿ƒæ…Œã€‚

14:04 - å¼µå¿ƒç†å¸«: é€™äº›éƒ½æ˜¯ç„¦æ…®çš„å…¸å‹ç—‡ç‹€ã€‚æˆ‘å€‘ä¾†è¨è«–ä¸€äº›æ‡‰å°ç­–ç•¥ã€‚

Q1: ä½ å¹³æ™‚æœ‰ä»€éº¼æ”¾é¬†çš„æ–¹æ³•å—ï¼Ÿ

14:05 - å­¸ç”ŸA: æˆ‘å–œæ­¡è½éŸ³æ¨‚ï¼Œä½†æœ€è¿‘é€£éŸ³æ¨‚éƒ½è½ä¸é€²å»ã€‚

14:06 - å¼µå¿ƒç†å¸«: æˆ‘å€‘å¯ä»¥å˜—è©¦ä¸€äº›æ›´ä¸»å‹•çš„æ”¾é¬†æŠ€å·§ï¼Œæ¯”å¦‚æ·±å‘¼å¸ç·´ç¿’ã€‚

14:07 - å­¸ç”ŸA: æ·±å‘¼å¸ï¼Ÿæˆ‘è©¦éï¼Œä½†æ„Ÿè¦ºæ²’ä»€éº¼ç”¨ã€‚

14:08 - å¼µå¿ƒç†å¸«: æ·±å‘¼å¸éœ€è¦æ­£ç¢ºçš„æ–¹æ³•ã€‚è®“æˆ‘æ•™ä½ ä¸€å€‹4-4-6å‘¼å¸æ³•...

14:10 - å­¸ç”ŸA: é€™æ¨£åšå—ï¼Ÿ(ç¤ºç¯„å‘¼å¸)

14:11 - å¼µå¿ƒç†å¸«: å¾ˆå¥½ï¼è¨˜ä½è¦æ…¢æ…¢ä¾†ï¼Œä¸è¦æ€¥ã€‚æˆ‘å€‘ä¸‹æ¬¡è¦‹é¢æ™‚å†è¨è«–å…¶ä»–æŠ€å·§ã€‚
"""
    
    integrated_rag = IntegratedMentalHealthRAG()
    
    # ä¿å­˜æœƒè©±æ–‡æª”
    session_file_path = "backend/test_session.txt"
    with open(session_file_path, 'w', encoding='utf-8') as f:
        f.write(session_text)
    
    print("ğŸ“ æœƒè©±æ–‡æª”å·²å‰µå»º")
    
    # æ¸¬è©¦æœƒè©±åˆ†å¡Š
    result = await integrated_rag.process_document_with_strategy(
        session_file_path,
        "test_session.txt",
        strategy=ChunkingStrategy.SESSION,
        chunk_size=300,
        overlap=50
    )
    
    if result["success"]:
        print(f"âœ… æœƒè©±åˆ†å¡ŠæˆåŠŸ")
        print(f"   - åˆ†å¡Šæ•¸é‡: {result['chunk_count']}")
        print(f"   - åˆ†å¡Šé¡å‹: {result['statistics']['chunk_types']}")
        
        print("\nğŸ“‹ æœƒè©±åˆ†å¡Šå…§å®¹é è¦½:")
        for i, chunk in enumerate(result['chunks'][:3]):
            print(f"\næœƒè©±åˆ†å¡Š {i+1}:")
            print(f"é¡å‹: {chunk.get('chunk_type', 'unknown')}")
            print(f"å…§å®¹: {chunk['text'][:100]}...")
    else:
        print(f"âŒ æœƒè©±åˆ†å¡Šå¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    # æ¸…ç†æ–‡ä»¶
    if os.path.exists(session_file_path):
        os.remove(session_file_path)
        print(f"\nğŸ§¹ æœƒè©±æ¸¬è©¦æ–‡ä»¶å·²æ¸…ç†")

async def main():
    """ä¸»å‡½æ•¸"""
    try:
        # åŸºæœ¬æ•´åˆæ¼”ç¤º
        await demonstrate_integration()
        
        # æœƒè©±åˆ†å¡Šæ¼”ç¤º
        await demonstrate_session_chunking()
        
        print("\nğŸ¯ ç¸½çµ:")
        print("1. èªç¾©åˆ†å¡Šæœ€é©åˆå¿ƒç†å¥åº·æ–‡æª”ï¼Œä¿æŒå…§å®¹å®Œæ•´æ€§")
        print("2. å±¤æ¬¡åˆ†å¡Šé©åˆçµæ§‹åŒ–æ–‡æª”ï¼Œå¦‚æŒ‡å—å’Œæ‰‹å†Š")
        print("3. æœƒè©±åˆ†å¡Šé©åˆè«®è©¢è¨˜éŒ„å’Œå°è©±æ–‡æª”")
        print("4. è‡ªé©æ‡‰åˆ†å¡Šå¯ä»¥æ™ºèƒ½é¸æ“‡æœ€ä½³ç­–ç•¥")
        print("5. å›ºå®šé•·åº¦åˆ†å¡Šä½œç‚ºå‚™ç”¨æ–¹æ¡ˆï¼Œç°¡å–®å¯é ")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")

if __name__ == "__main__":
    asyncio.run(main())
